{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca229b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb9925a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from omegaconf import OmegaConf\n",
    "from cai.plmodule.module import HPASSCModule\n",
    "from cai.pldatamodule.datamodule import HPASSCDataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c5e4a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = \"../models/epoch=49-step=3400.ckpt\"\n",
    "cfg = OmegaConf.to_container(OmegaConf.load(\"/mnt/yando/Users/yando/Experiments/cai/outputs/2022-12-12/23-25-29/.hydra/config.yaml\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2c8bb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_transforms = [\n",
    "        [\n",
    "            {\n",
    "                \"module\" : \"tv_transforms\",\n",
    "                 \"name\" : \"CenterCrop\",\n",
    "                 \"kwargs\" : {\n",
    "                     \"size\" : [1568, 1568]\n",
    "                 }\n",
    "            },\n",
    "            {\n",
    "                \"module\" : \"tv_transforms\",\n",
    "                \"name\" : \"Normalize\",\n",
    "                \"kwargs\" : {\n",
    "                    \"mean\" : [0.06438801, 0.0441467, 0.03966651, 0.06374957],\n",
    "                    \"std\" : [0.10712028, 0.08619478, 0.11134183, 0.10635688]\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"module\" : \"tv_transforms\",\n",
    "                \"name\" : \"Resize\",\n",
    "                \"kwargs\" : {\n",
    "                    \"size\" : [224, 224]\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "        [\n",
    "            {\n",
    "                \"module\" : \"tv_transforms\",\n",
    "                 \"name\" : \"CenterCrop\",\n",
    "                 \"kwargs\" : {\n",
    "                     \"size\" : [1568, 1568]\n",
    "                 }\n",
    "            },\n",
    "            {\n",
    "                \"module\" : \"tv_transforms\",\n",
    "                \"name\" : \"Normalize\",\n",
    "                \"kwargs\" : {\n",
    "                    \"mean\" : [0.06438801, 0.0441467, 0.03966651, 0.06374957],\n",
    "                    \"std\" : [0.10712028, 0.08619478, 0.11134183, 0.10635688]\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"module\" : \"nn\",\n",
    "                \"name\" : \"Unfold\",\n",
    "                \"kwargs\" : {\n",
    "                    \"kernel_size\" : [224,224],\n",
    "                    \"stride\" : 224\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d9eef53",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg[\"module\"][\"loss_fn\"][\"module\"] = \"nn\"\n",
    "cfg[\"datamodule\"][\"dataset\"][\"transforms\"] = inference_transforms[1]\n",
    "cfg[\"datamodule\"][\"dataloader\"][\"batch_size\"]=4\n",
    "cfg[\"datamodule\"][\"dataloader\"][\"num_workers\"]=16\n",
    "cfg[\"datamodule\"][\"dataloader\"][\"prefetch_factor\"]=4\n",
    "del cfg[\"datamodule\"][\"dataloader\"][\"sampler\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "538f8352",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\"\n",
    "module = HPASSCModule.load_from_checkpoint(ckpt, **cfg[\"module\"]).to(device)\n",
    "datamodule = HPASSCDataModule(**cfg[\"datamodule\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a24d1c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "module.eval()\n",
    "datamodule.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1e5da6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = datamodule.train_dataset[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8f4ca8c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image = image.permute(1,0).view(49, -1, 224, 224) if len(image.shape) < 3 else image[None, :, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cb1d3039",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([49, 19]), 12, torch.Size([49, 12, 197, 197]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = image.to(device)\n",
    "output = module.reduce_channel(image)\n",
    "output = module.model(output, output_attentions=True)\n",
    "output.logits.shape, len(output.attentions), output.attentions[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7482295e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.sort(\n",
       "values=tensor([0.2268, 0.1832, 0.1250, 0.0762, 0.0750, 0.0626, 0.0621, 0.0618, 0.0609,\n",
       "        0.0597, 0.0580, 0.0559, 0.0523, 0.0509, 0.0492, 0.0463, 0.0430, 0.0417,\n",
       "        0.0405], device='cuda:0', grad_fn=<SortBackward0>),\n",
       "indices=tensor([ 0, 16,  8, 13, 10, 17, 15,  5, 14,  4,  6,  3,  7,  1,  2, 12,  9, 18,\n",
       "        11], device='cuda:0'))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(output.logits, dim=1).max(dim=0).values.sort(descending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfeae8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tiles, nh, _, _ = output.attentions[-1].shape\n",
    "attentions = output.attentions[-1][:, :, 0, 1:]\n",
    "attentions = (attentions * (attentions > 0.003)).view(n_tiles, nh, -1)\n",
    "for head in range(nh):\n",
    "    fig, ax = plt.subplots(1, 2)\n",
    "    grid = vutils.make_grid(attentions[:, head,:].view(-1, 1, 14, 14), nrow=7, ncol=7, padding=0, normalize=True)\n",
    "    image_grid = vutils.make_grid(image, nrow=7, ncol=7).permute(1,2,0).mean(dim=-1).cpu()\n",
    "    grid = transforms.functional.resize(grid, (1568, 1568)).cpu().permute(1,2,0)\n",
    "    ax[1].imshow(grid[:,:,0], cmap=plt.cm.rainbow)\n",
    "    ax[0].imshow(image_grid)\n",
    "    ax[0].set_axis_off()\n",
    "    ax[1].set_axis_off()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b6c96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from tqdm import tqdm\n",
    "num_classes = 19\n",
    "num_layers = 12\n",
    "num_tiles = 49\n",
    "grad = torch.zeros((num_layers, *output.attentions[-1].shape)).to(device)\n",
    "for cla, layer, tile in tqdm(product(range(num_classes), range(num_layers), range(num_tiles)), total=num_classes * num_layers * num_tiles):\n",
    "    #grad[cla, layer] += torch.autograd.grad(output.logits[tile, cla], output.attentions[layer], retain_graph=True)[0] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f18c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.0001\n",
    "n_tiles, nh, _, _ = output.attentions[-1].shape\n",
    "attentions = (output.attentions[-1] * grad)[:, :, 0, 1:]\n",
    "attentions = (attentions * (attentions > threshold)).view(n_tiles, nh, -1)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "grid = vutils.make_grid(attentions.max(dim=1).values.view(-1, 1, 14, 14), nrow=7, ncol=7, padding=0).cpu().permute(1,2,0)\n",
    "image_grid = vutils.make_grid(image.cpu(), nrow=7, ncol=7).permute(1,2,0).mean(dim=-1)\n",
    "ax[1].imshow(grid[:,:,0], cmap=plt.cm.rainbow, interpolation=\"bilinear\")\n",
    "ax[0].imshow(image_grid)\n",
    "ax[0].set_axis_off()\n",
    "ax[1].set_axis_off()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4dd49d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4361 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from torchmetrics.classification import MultilabelPrecisionRecallCurve\n",
    "mlprc = MultilabelPrecisionRecallCurve(19, thresholds=100).to(device)\n",
    "dataloader = datamodule.train_dataloader()\n",
    "\n",
    "for batch in tqdm(dataloader):\n",
    "    image, label = batch\n",
    "    n_batch = image.shape[0]\n",
    "    image = image.permute(0,2,1).view(n_batch, 49, -1, 224, 224).to(device)\n",
    "    output = module.reduce_channel(image.reshape(-1, 4, 224, 224))\n",
    "    output = module.model(output)\n",
    "    output.logits = output.logits.reshape(n_batch, 49, -1)\n",
    "    mlprc.update(torch.sigmoid(output.logits).mean(dim=1), label.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d06c2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_, recall_, thresholds_ = mlprc.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7e50dbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_ = 2 * precision_ * recall_ / (precision_ + recall_ + torch.finfo(torch.float32).eps)\n",
    "best_f1 = torch.max(f1_, dim=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.5 ('aidev-3.7': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "ed7fa40a647af8769936bf3b7627dbc5f94fc520a514528562a614f30b0afd6b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
